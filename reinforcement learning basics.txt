참고 : https://rubikscode.net/2021/07/13/deep-q-learning-with-python-and-tensorflow-2-0/
'''
1.1 강화 학습의 기초
    - 어떠한 환경에서 특정 행동을 수행하는 에이전트를 관찰하여 해당 행동에서 얻는 보상을 기반으로 행동을 모델링한다.

    +) 지도 학습에서 에이전트는 특정 입력을 일부 출력에 매핑 하는 방법을 배운다. 예상 출력이 제공되기 때문에 수행 방법을 학습하기 쉽다.
    +) 비지도 학습에서 입력 데이터만 제공되며, 구조화 되어있찌 않은 데이터에서 패턴을 찾으려고 하여 학습한다.
    +) 강화 학습에서는 숨겨진 패턴을 찾으려고 하지 않으면서, 받는 보상을 최대화하려고 한다.

1.2 강화 학습의 주요 요소
    - 보상
    - 정책
    - 가치 함수

    1.2.1 보상
    - '에이전트의 목표'를 말하며 모든 시간 단계에서 모든 행동 후에 환경은 상태를 변경하고 에이전트에게 숫자의 형태로 보상을 제공한다.
    - 에이전트는 행동을 통해 직접적으로 더 나은 보상을 얻을 수도 있고, 환경의 상태를 변화시켜 간접적으로 더 나은 보상을 얻을 수도 있습니다.
    - 간단히 말해서 보상은 행동의 좋고 나쁨을 정의하고, 에이전트는 시간이 지남에 따라 보상을 최대화 하려고 합니다.

    1.2.2 정책
    - 에이전트가 특정 환경 상태에서 수행할 작업을 정의함.
    - 환경 상태를 에이전트의 작업에 매핑 합니다.
    - 얼핏 보면 정책이 단순한 테이블이나 함수처럼 보일 수 있습니다. 그러나 상황은 그보다 훨씬 더 복잡할 수 있으며 정책은 확률론적일 수 있다.

    1.2.3 값 함수
    - 보상과 달리 '가치 함수'는 에이전트의 장기 목표를 정의함.
    - '상태 값'은 해당 상태를 시작으로 향후에 보상 에이전트가 누적할 수 있는 양
    - '값'은 보상의 예측을 나타냄. 
    - 에이전트의 관점에서 보상은 주요 목표이고 가치는 부차적입니다. 이는 환경의 확률론적 특성 때문입니다. 그러나 다음에 수행할 작업에 대한 결정은 항상 값 을 기반으로 수행됩니다 . 즉, 에이전트는 항상 가장 높은 값을 가진 상태에 들어가려고 합니다. 이는 장기적으로 더 많은 보상을 얻을 수 있음을 의미하기 때문

1.3 마르코프 결정 과정
    - 시간 t에서의 상태는 t−1에서의 상태에만 영향을 받는다"는 first-order Markov assumption을 기반으로 고안되었다
    - MDP는 의사 결정의 결과가 부분적으로는 무작위이고 부분적으로는 의사 결정자가 제어하는 모델링 의사 결정에 사용됩니다.
    - 환경이 존재할 수 있는 상태의 수와 에이전트가 할 수 있는 행동의 수는 유한
    - (참고) 화면 캡처 
    - Markov Decision Process는 4가지 요소( S , A, Pa, Ra ) 의 튜플 입니다.
        -S  – 상태 집합을 나타냅니다 . 각 시간 단계 t 에서 에이전트는 환경 상태 St 를 얻습니다. 여기서 St ∈ S 입니다. 
        -A  – 에이전트가 수행할 수 있는 일련의 작업 을 나타냅니다. 각 시간 단계 t 에서 수신된 상태 St 에 따라 에이전트는 At ∈ A(St) 인 작업을 수행하기 로 결정 합니다 . A(St) 는 St 상태에서 가능한 일련의 작업을 나타냅니다.
        -Pa  – 어떤 상태 s 에서의 행동 이 시간 단계 t 를 초래할 확률 을 나타내며, 시간 단계 t+1 에서 상태 s' 를 초래할 확률을 나타 냅니다
        -Ra –  또는 더 정확하게 Ra(s, s') 는 조치 a 의 결과로 상태 s 에서 상태 s' 로 이동한 후 받는  예상 보상 을 나타냅니다

        +) S: state의 집합을 의미한다. State는 바둑에서 바둑판에 돌이 어떻게 놓여져 있는가를, 미로를 탈출하는 문제에서는 현재의 위치를 나타낸다.
        +) P: 각 요소가 p(s′|s)=Pr(St+1=s′|St=s)인 집합이다. p(s′|s)는 현재 상태 s에서 s′으로 이동할 확률을 의미하며, transition probability라고 한다.
        +) R: 각 요소가 r(s)=E[Rt+1|St=s]인 집합이다. r(s)는 state s에서 얻는 reward를 의미한다.
        +) γ: 즉각적으로 얻는 reward와 미래의 얻을 수 있는 reward 간의 중요도를 조절하는 변수이다. 주로 [0, 1] 사이의 값을 가지며, discount factor라고 한다.

        = Pa(s,s') = P(s'=St+1|St = s, At = a)
        '
        '
        '


2. Q-Learning 직관
    - Q-Learning 은 강화 학습에 대한 소위 표 형식 솔루션의 일부이거나 더 정확하게는 시간차 알고리즘의 한 종류 입니다 .

2.1 시차 학습
    - 부분적으로 다른 추정치를 기반으로 추정치를 학습합니다
    - 부트스트래핑
    - 시간차의 가장 간단한 형태는 일반적으로 TD(0) 으로 표시됩니다 . 수학 공식으로 표시됩니다.
        - V(St) = V(St) + α[Rt+1 + γV(St+1)-V(St)]
        - 여기서 α는 학습률입니다. 이는 이 접근 방식이 다음 시간 단계 t+1 까지 대기 하고 해당 시간 단계의 보상 및 추정 값을 사용하여 시간 단계 t 의 값을 업데이트한다는 것을 의미합니다 . TD(0) 은 다음과 같이 수행됩니다.
            1. 임의 의 상태 집합 S 에서 각 상태에 대한 값을 초기화 합니다. V(s) = n, ∀s ∈ S .
            2. 정책 π에 의해 정의된 상태 A(들) 에 대해 정의된 일련의 작업에서 작업 a 를 선택 합니다.
            3. 작업 수행 _
            4. 보상 R 과 다음 상태 s' 를 관찰하십시오 .
            5. 다음 공식을 사용하여 상태에 대한 업데이트 값: V(s) ← V(s) + α [R + γV(s') − V(s)]
            6. 터미널 상태에 도달할 때까지 각 시간 단계에 대해 2-5단계를 반복 합니다.
            7. 각 에피소드에 대해 2-6단계를 반복 합니다.

2.2 Q-러닝 기초
    - Q-Learning 은 한 단계 더 나아가  정책 π – q 에 따라  상태  s 에서 조치 a 를 취하는 전술한 값을 추정합니다 
    - 기본적으로 이 접근 방식의 결정은 상태-값 쌍이 아닌 상태 -동작 쌍의 추정을 기반으로 합니다.
    - Q(St,At) = Q(St,At) + α[Rt+1 + γQ(St+1,a) - Q(St,At)] 
    - 특정 상태-동작 조합에 대한 Q-값은 해당 상태에서 취한 동작의 품질로 관찰 할 수 있습니다 . 보시다시피 정책은 여전히 ​​방문 및 업데이트되는 상태-동작 쌍을 결정하지만 그 이상은 아닙니다.
    - 이러한 모든 Q-값 은 상태에 대한 행과 작업에 대한 열이 있는 행렬 인 Q-Table 내부에 저장됩니다 . 이 테이블은 에이전트에 의해 업데이트되며 다음과 같이 표시됩니다.
        1. Q-Table 의 모든 Q-Value 를 임의적으로 초기화 하고 단말 상태의 Q 값을 0으로 초기화합니다. Q(s, a) = n, ∀s ∈ S , ∀a ∈ A(s) Q(terminal-state, ·) = 0
        2. 정책 π에 의해 정의된 상태 A(들) 에 대해 정의된 일련의 작업에서 작업 a 를 선택 합니다.
        3. 작업 수행 _
        4. 보상 R 과 다음 상태 s' 를 관찰하십시오 .
        5. 상태 s' 에서 가능한 모든 작업에 대해 Q-값 이 가장 높은 것을 선택하십시오( a ') .
        6. 다음 공식을 사용하여 상태에 대한 업데이트 값: Q(s, a) ← Q(s, a) + α [R + γQ(s', a') − Q(s, a)]
        7. 터미널 상태에 도달할 때까지 각 시간 단계에 대해 2-5단계를 반복 합니다.
        8. 각 에피소드에 대해 2-6단계를 반복 합니다.
    - 얼마간의 시간과 행동에 대한 충분한 무작위 탐색 후에, Q-값 은 이전에 언급한 행동-값 함수 로 에이전트를 제공하는 데 수렴하는 경향이 있습니다. 주목해야 할 중요한 점은 때때로 과적합 을 멈추기 위해 추가적인 제약 을 추가한다는 것 입니다.
    - 기본적으로 우리는 새로운 작업을 탐색하고 더 나은 솔루션을 내놓을 것인지 또는 이미 학습된 경로를 사용할 것인지 정의 하는 값 엡실론 을 사용합니다. 이 매개변수는 새 옵션 탐색 과 이미 학습된 옵션 활용 간의 관계를 정의합니다 .


3. Python으로 Q-Learning 구현

3.1 전제 조건

3.2 OpenAI 환경

3.3 파이썬으로 구현하기

4. 깊은 Q-Learning 직관

5. Python 및 TensorFlow를 사용한 심층 Q-Learning 구현

결론 

-> 우리는 Deep Q-Learning 을 살펴보았다. 이것은 신경망을 활용하는 첫 번째 유형의 강화 학습이다. 
'''